# 项目简介

## 概述
本项目使用 KerasNLP 对 Gemma 模型进行 LoRA 微调，为了快速验证微调的效果，仅在数据集的一个小子集上进行了一个周期的微调，并使用了较低的 LoRA 秩值。取得了显著的改进效果。

## 时间
2024.09.15-2024.09.16

## 主要工作和收获
- 掌握了用 LoRA 技术改进微调的原理和基本使用
- 掌握了用 KerasNLP 微调 Gemma 模型的基本操作

## 技术栈
KerasNLP，Gemma，LoRA

##  数据集
databricks-dolly-15k

## 结果
使用LoRA微调后，在两个示例上结果有明显改进

## kaggle地址
[Fine-tune Gemma models in Keras using LoRA](https://www.kaggle.com/code/chenxucool/fine-tune-gemma-models-in-keras-using-lora)

## csdn地址
[【AI小项目5】使用 KerasNLP 对 Gemma 模型进行 LoRA 微调](https://blog.csdn.net/weixin_43221845/article/details/142299446?sharetype=blogdetail&sharerId=142299446&sharerefer=PC&sharesource=weixin_43221845&spm=1011.2480.3001.8118)

## 参考
[Fine-tune Gemma models in Keras using LoRA](https://www.kaggle.com/code/nilaychauhan/fine-tune-gemma-models-in-keras-using-lora)
[Practical Tips for Finetuning LLMs Using LoRA](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms)

